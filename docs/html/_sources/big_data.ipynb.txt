{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data\n",
    "\n",
    "## What *Is* Big Data?\n",
    "\n",
    "Big Data is probably the only term that can compete with \"data science\" for frequency of use despite the fact no one means the same thing when they say it. In many contexts, for example, \"big data\" just refers to large datasets. \n",
    "\n",
    "In this course, however, we'll use a much more precise definition: data is \"big\" if it doesn't fit into RAM (which we'll refer to here as **main memory** for reasons that will become clear later. ). \n",
    "\n",
    "## Why does it matter if data fits into main memory?\n",
    "\n",
    "Before we get into big data and strategies for working with it, we need to take a short digression into computer architecture. \n",
    "\n",
    "When you computer executes code -- for example, adding up all the entries in a vector -- it doesn't all happen in one place. Your computer stores data (like your vector) in main memory (again, this is the term used for what you buy as RAM) where it has several gigabytes of space to put things. But actual mathematical operations happen in the processor, which is physically located in a different place in your computer. So to add two numbers, you computer must move those numbers from main memory into a *tiny* set of memory banks inside your processor from which they can be accessed, added, and the result stored. (Most modern processor cores have 16 innermost memory slots called \"registers\", each of which can only store only 64 bits of data -- i.e. a single number.) \n",
    "\n",
    "This is important because in data science, moving data back and forth from memory takes *much* longer than doing actual calcuations. Indeed, in a normal data scienc workflow, moving data back and forth from memory is a *huge* portion of what takes up the time your computer spends \"working\", not doing actual calculations.\n",
    "\n",
    "This makes memory management important in general, but the problem becomes much acute when you can't store your data in main memory. That's because while you *can* store data on your harddrive, and move data directly from your harddrive to your processor and back, that process is *unimaginably* slow.\n",
    "\n",
    "How much slower? Data can move from main memory to the processor at a rate ~1,000 times faster than from a spinning harddrive, over 100 times faster than from SATA SSDs, and over 25 times faster than from m2 SSDs. (speeds estimted from transfer speed estimates [here](https://en.wikipedia.org/wiki/Memory_hierarchy#Examples)). \n",
    "\n",
    "But even those transfer speeds are only half the story -- hard drives also have higher *latency*, meaning that with harddrives, there's a much longer lag between when you *ask* for data when the data transfer begins than with main memory. How much slower? This table re-scales actual latency times (the time between when you ask for a byte of data and when it arrives at the processor) to human time scales (don't worry about the cache entries yet) ([from this excellent post](https://www.formulusblack.com/blog/compute-performance-distance-of-data-as-a-measure-of-latency/)). \n",
    "\n",
    "![human_time](images/human_time.png)\n",
    "\n",
    "\n",
    "As the table shows, if we were to pretend that a single tick of your processor (essentially how long it takes to add two numbers) took 1 second, then stopping to get *one byte* of data from main memory would take several minutes. And if we wanted to get data from a spinning harddrive, we'd have to wait between 11 days and *four months*. SSDs obviously perform much better than physical harddrives with spinning platters, but even those take hours to days. *And that's just to get the first byte!* There are *also* the differences in transfer rates once data starts flowing!\n",
    "\n",
    "In other words, when working with large datasets, moving around your data is often the most expensive thing you will do, and the penalities for *not* being able to fit your data into main memory mean that something that might take minutes if you could do it with data in main memory could take days. \n",
    "\n",
    "### OK, what's this cache stuff?\n",
    "\n",
    "Because of how expensive it is to move data back and forth from main memory, you computer has a set of intermediate memory banks between main memory and your processor called *caches*. Caches try to predict what data you might use in the future, and keep it close to the processor so it can be accessed faster. Caches can substantially improve performance, but as a programmer you don't actually get to control what gets put into cache -- your cache is managed by the operating system. There are some tricks for trying to use data in ways that are more likely to result in your computer being able to use data stored in cache (in general, your computer fills the cache with (a) data you've used recently, and (b) data that is stored *near* the data you've used recently, so doing everything you can with the same object at once *can* help a little), but honestly they're just not something you should worry about. I just wanted to demystify \"cache\" for when you come across it. \n",
    "\n",
    "## Types of Big Data\n",
    "\n",
    "In the early 2000s, the term big data started getting thrown around for anything that you can't fit into main memory. But in the early ~2010s, the term also started being used for data you can't actually fit onto a single *computer*. Data on these two scales are very different, so in the rest of this tutorial I'll use this terminology: \n",
    "\n",
    "- **Big Data**: data that is too big to load and work with entirely in main memory\n",
    "    - hundreds of gigabytes of data to a few terabytes of data\n",
    "- **Very Big Data**: data that is too big to load and work with entirely in RAM, and also too big to store on a single computer's hard drive.\n",
    "    - peta-bytes of data\n",
    "\n",
    "Note this distinction is my own -- when you go into the world, you will find big data sometimes means using unusually large datasets, sometimes means data that doesn't fit into ram, and sometimes means data that won't fit on a single computer's harddrive. But we'll use this here. \n",
    "\n",
    "## How do I know if my data is fitting into RAM?\n",
    "\n",
    "Oddly, this is not actually a straight-forward question for two reasons:\n",
    "\n",
    "1.  **Programs often copy data in order to manipulate it.** When you, say, sort an array, what your computer actually does is create a new empty array, then move over entries from your original array into the new array one entry at a time (in sorted order). While it does then erase the original array, while it's working it effectively has two copies of your array. As a result, the fact that you have 16gb of RAM doesn't mean you can easily work with a 14gb file. As a general rule, you need *at least* two times as much memory as your file takes up when first loaded. \n",
    "2. **The size of data changes as you change its format.** For example, if you're reading data from an inefficient file format (like a .csv text file), it may be the case that a file that is 10gb on disk may actually be only a few gb once you load it into a program like Python that knows how to store data more efficiently. \n",
    "\n",
    "### So how do I check to see if my data is fitting in memory?\n",
    "\n",
    "If your program starts using more space than you have main memory, your operating system will usually just start using your harddrive for extra space without telling you (this is called \"virtual memory\", and is nice in that it prevents your computer from crashing, though it will slow stuff down a lot). As a result, you won't always get an error message if you try and load a file to is much bigger than main memory. \n",
    "\n",
    "To avoid this, use one of the tools provided by your operating system to monitor whether it's using the harddrive:\n",
    "\n",
    "- '''OSX:''' OSX comes with a program called [Activity Monitor](https://support.apple.com/en-us/HT201464#memory) in the Applications > Utilities folder. The memory tab of Activity Monitor has a graph at the bottom called \"Memory Pressure\". If this is green, you're good. If it's yellow or red, you're actively going back and forth to your harddrive. (Note your computer may be using \"virtual memory\" without affecting performance if it's just storing away things you aren't actively using).\n",
    "- '''Windows:''' Windows has a program called [\"Resource Monitor\"](http://en.wikipedia.org/wiki/Resource_Monitor) that shows data on memory use. If the \"Hard Faults\" graph under the memory tab is spiking, or you get near 100% Used Physical Memory, your computer is probably using the harddrive.\n",
    "\n",
    "## Strategies for Working with Big Data\n",
    "\n",
    "If you have big data, you basically have three options:\n",
    "\n",
    "- Format and trim your data so it fits in memory or get more RAM;\n",
    "- Chunk your data yourself;\n",
    "- Use specialized software.\n",
    "\n",
    "\n",
    "## Avoid Spillover\n",
    "Working with data that doesn't fit into RAM is a ''headache''. If there's anyway you can avoid it, you should, which can potentially be accomplished via two methods:\n",
    "\n",
    "- '''Drop variables or store them more efficiently:''' If you're used to small datasets, you may be in the habit of always carrying around all the variables in a survey or dataset. But if you only keep the variables you absolutely need, or find ways to store them more efficiently, you can often stay within your size limits. For example, String variables take up lots of space -- if you can, turn them into numeric variables (see [[Data Types]] for more on data types).\n",
    "- '''Get more RAM''': RAM isn't cheap, but nor is your time, and buying more RAM can save you an amazing amount of trouble. And if you can't afford to buy your own, you may find that it's cheaper to rent a computer with more RAM from cloud services like Amazon, which are often surprisingly affordable! For more, see the page on [[Cloud Computing Resources]].\n",
    "\n",
    "## Chunking\n",
    "One way to manipulate large datasets is by chunking -- loading your data into RAM one chunk (say, 1 million rows) at a time, manipulating it, and then appending it to a new data file. This is relatively straightforward for operations that act only on single rows -- like recoding variables -- but a little harder when you start doing things that involve multiple rows at once (like sorting, or taking averages over groups).\n",
    "\n",
    "\n",
    "## Big Data Tools\n",
    "\n",
    "\n",
    "### Easy to work with, but somewhat limited: `dask`\n",
    "\n",
    "`Dask` is a new tool written for working with data that doesn't fit into memory (and parallelizing operations) for Python. It was written to basically work just like `pandas`, so it's quite easy to get started using. The only catch is that it only supports a certain number of functions at this point, so it will do a lot, but not everything.\n",
    "\n",
    "* `Dask Overview <https://www.youtube.com/watch?v=1kkFZ4P-XHg>`_\n",
    "* `Long Tutorial <https://www.youtube.com/watch?v=ieW3G7ZzRZ0>`_\n",
    "\n",
    "Dask is probably best for *big data*, but I'm not sure it can really handle *very big data* (data you can't put on one computer's harddrive). The authors aim to fix that, but I'm not sure the system is there yet. \n",
    "\n",
    "### More involved, but more powerful: `pyspark`\n",
    "\n",
    "\n",
    "`Spark` is like the second generation of a platform called Hadoop for working with data across lots and lots of software. `pyspark` is a great tool for manipulating `spark` using python. `spark` is *very* powerful, but it's not a tool created for Python users, it's an entire computing system you'll have to learn about to use. \n",
    "\n",
    "Though there are attempts to make `PySpark` easier for `pandas` users to use, like `Sparkling Pandas` (`tutorial here <https://www.youtube.com/watch?v=borv_KMI9Ac>`_)\n",
    "\n",
    "Unlike `Dask`, `Spark` and `PySpark` were built not just for *big data* (data that doesn't fit in RAM), but specifically for *very big data* (data that won't even fit on a single computer's hard drive). So if you have *very big data*, this is probably the way to go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
