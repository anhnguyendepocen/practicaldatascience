{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Your Own Linear Regression Model\n",
    "\n",
    "One task that you will almost certainly be required to do other data science courses (especially if you are a MIDS student) is to write up some of your statistical / machine learning models from scratch. This can be a very valuable exercise, as it ensures that you understand what is actually going on behind the scenes of the models you use ever day, and that you don't just think of them as \"black boxes\". \n",
    "\n",
    "To get a little practice doing this, today you will be coding up your own linear regression model! \n",
    "\n",
    "(If you are using this site but aren't actually in this class, you are welcome to skip this exercise if you'd like -- this is more about practicing Python in anticipation of the requirements of other courses than developing your applied data science skills.) \n",
    "\n",
    "There are, broadly speaking, two approaches you can take to coding up your own model: \n",
    "\n",
    "1. you can write the model by defining a new function, or \n",
    "2. you can write the model by defining a new class with associated methods (making a model that works the way a model works in `scikit-learn`). \n",
    "\n",
    "Whether you do 1 or 2 is very much a matter of choice and style. Approach one, for example, is more consistent with what is called a *functional* style of programming, while approach two is more consistent with an *object-oriented* style of programming. Python can readily support both approaches, so either would work fine. \n",
    "\n",
    "In these exercises, however, I will ask you to use approach number 2 as this *tends* to be the more difficult approach, and so practicing approach 2 will be extra useful in preparing you for other classes (HA! Pun...). In particular, our goal is to implement a linear regression model that has the same \"initialize-fit-predict-score\" API (application programming interface -- a fancy name for the methods a class supports) as `scikit-learn` models. That means your model should be able to do all of the following:\n",
    "\n",
    "1. **Initialize** a new model. \n",
    "2. **Fit** a linear model when given a numpy vector (`y`) and a numpy matrix (`X`) with the syntax `my_model.fit(X, y)`. \n",
    "3. **Predict** values when given a new `numpy` matrix (`X_test`) with the syntax `my_model.predict(X_test)`. \n",
    "4. Return the **model coefficients** through the property `my_model.coefficients` (not quite what is used in `sklearn`, but let's use that interface). \n",
    "\n",
    "Also, bear in mind that throughout these exercises, we'll be working in `numpy` instead of `pandas`, just as we do in `scikit-learn`. So assume that before using your model, your user has already converted their data from `pandas` into `numpy` arrays. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1)** Define a new Class called `MyLinearModel` with methods for `__init__`, `fit`, `predict`, and an attribute for `coefficients`. For now, we don't need any initialization *arguments*, just an `__init__` function. \n",
    "\n",
    "As you get your code outline going, start by just having each method `pass`:\n",
    "\n",
    "```python\n",
    "def my_method(self):\n",
    "    pass\n",
    "```\n",
    "\n",
    "This will allow your methods to run without errors (they just don't do anything). Then we can double back to each method to get them working one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we define just the skeleton. \n",
    "# Then we can start filling in the elements. \n",
    "\n",
    "class MyLinearModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize!\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Fit the model using regressors X and outcomes y\n",
    "        pass\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Make predictions on X_test\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2)** Now define your `fit` method. This is the method that should actually run your linear regression. \n",
    "\n",
    "Note that once you have written the code to do a linear regression, you'll need to put your outputs (your coefficents) somewhere. I recommend making an attribute for your class where you can store your coefficients. \n",
    "\n",
    "(As a reminder: the normal multiply operator (`*`) in `numpy` implies scalar multiplication. Use `@` for matrix multiplication). \n",
    "\n",
    "**HINT:** Remember that linear regressions usually include a constant term. As in most packages, you should assume that users want this included, even if there isn't a vector of 1s in their `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyLinearModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize!\n",
    "        self.coefficients = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Fit the model using regressors X and outcomes y\n",
    "        X_transposed = np.transpose(X)\n",
    "        self.coefficients = np.linalg.inv(X_transposed @ X) @ X_transposed @ y\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        # Make predictions on X_test\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(3)** As you write code, it is good to test your code as you work. With that in mind, let's create some toy data. First, create a 100 x 2 matrix where each column is normally distributed. Then create a vector `y` that is a linear combination of those two columns plus a vector of normally distributed noise. \n",
    "\n",
    "In other words, we want to create data where we *know* exactly what coefficients we should see so when we test our regression, we know if the results are accurate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.73457555,  6.47066649],\n",
       "       [ 7.01730945,  4.88614188],\n",
       "       [ 3.40504011,  5.56196447],\n",
       "       [ 9.12783464,  9.36160644],\n",
       "       [ 4.16366345,  6.0352925 ],\n",
       "       [ 7.45385804, 10.60911613],\n",
       "       [10.13338834,  4.81598829],\n",
       "       [ 7.48921071,  6.59067317],\n",
       "       [ 8.47994778,  5.00960044],\n",
       "       [ 9.18026357,  3.80770368],\n",
       "       [ 5.67417086,  8.33234133],\n",
       "       [ 6.89822477,  5.56569215],\n",
       "       [ 4.95754607,  9.09125823],\n",
       "       [ 8.56938795,  5.87821552],\n",
       "       [ 5.47119479,  6.78291183],\n",
       "       [ 5.10084457,  9.95365349],\n",
       "       [ 7.78658491,  4.43920385],\n",
       "       [11.37790777,  6.33509597],\n",
       "       [ 8.34192528,  4.62859172],\n",
       "       [ 6.86635797,  6.71586121],\n",
       "       [ 6.63563464,  9.55009445],\n",
       "       [ 7.99770803,  8.22214816],\n",
       "       [ 4.57188023,  5.8372884 ],\n",
       "       [ 8.16145962,  6.11153637],\n",
       "       [ 7.99904703,  4.9361522 ],\n",
       "       [ 6.75292303,  8.42781277],\n",
       "       [11.17441834,  7.1563693 ],\n",
       "       [ 5.60238077,  7.17944292],\n",
       "       [ 4.93738013,  9.08491602],\n",
       "       [ 6.81115306,  7.63077245],\n",
       "       [ 7.469684  ,  3.71680127],\n",
       "       [ 6.84080936,  9.40224184],\n",
       "       [ 4.74547573, 10.00451187],\n",
       "       [ 9.07805768, 10.21694183],\n",
       "       [ 4.13794415,  9.28027414],\n",
       "       [ 6.67232241,  5.2685632 ],\n",
       "       [ 7.99173324,  8.84631809],\n",
       "       [ 4.56833524,  8.99049619],\n",
       "       [ 7.4352305 ,  7.70216182],\n",
       "       [ 6.50325405,  6.31923924],\n",
       "       [ 6.4739645 ,  7.13576807],\n",
       "       [ 7.94925736,  4.93269575],\n",
       "       [ 7.12854008,  2.85276495],\n",
       "       [ 5.65951399,  5.98536766],\n",
       "       [11.1219767 ,  5.17534782],\n",
       "       [ 9.33172556,  6.95275583],\n",
       "       [ 6.63398138,  4.0567303 ],\n",
       "       [ 8.26437214,  4.80253181],\n",
       "       [ 9.86829446,  4.63274242],\n",
       "       [ 5.15857909,  5.95732184],\n",
       "       [ 6.89002911,  4.17593475],\n",
       "       [ 5.32390625,  8.90347628],\n",
       "       [11.90769415,  4.55146652],\n",
       "       [ 6.94284895,  8.40580839],\n",
       "       [ 8.05979988,  5.92044359],\n",
       "       [ 4.8127505 ,  5.08126879],\n",
       "       [ 7.82160746,  5.83356929],\n",
       "       [10.50222913,  6.32511734],\n",
       "       [ 6.62075786,  4.00870898],\n",
       "       [ 5.41768454,  6.11722574],\n",
       "       [10.28238418,  8.95700996],\n",
       "       [ 6.55156202,  6.89280919],\n",
       "       [ 7.46316278,  7.3173358 ],\n",
       "       [ 4.98963561,  6.12683217],\n",
       "       [ 8.08116516,  3.80591996],\n",
       "       [10.43421858,  7.71562249],\n",
       "       [ 6.33757039,  7.65143039],\n",
       "       [ 5.16718441,  6.3208542 ],\n",
       "       [10.93621225,  7.37174031],\n",
       "       [ 9.79889018,  6.26458368],\n",
       "       [ 9.44597733,  8.89332554],\n",
       "       [ 3.79074546,  6.43023243],\n",
       "       [ 7.03114708,  7.99098589],\n",
       "       [ 4.85583771,  4.30524603],\n",
       "       [ 7.90959893,  5.60610146],\n",
       "       [ 4.62756866,  9.90988926],\n",
       "       [ 7.94227701,  9.09388336],\n",
       "       [ 4.99454833,  7.92747789],\n",
       "       [ 5.82575137,  4.97700787],\n",
       "       [ 4.33105756,  6.10021401],\n",
       "       [ 7.30354795,  7.56989385],\n",
       "       [11.30068201, 10.71513995],\n",
       "       [ 9.45078749,  7.9339044 ],\n",
       "       [11.07052028,  6.41066314],\n",
       "       [ 7.88020648,  6.03883566],\n",
       "       [ 4.66072492,  4.96740274],\n",
       "       [ 6.81503375,  6.36879184],\n",
       "       [ 7.86707753,  9.14635708],\n",
       "       [ 7.26112591,  7.37816177],\n",
       "       [ 6.14248749,  5.54950062],\n",
       "       [ 8.29564963,  4.78772801],\n",
       "       [ 6.98771809,  6.53521847],\n",
       "       [ 5.60245662,  4.23809588],\n",
       "       [ 7.32578443,  8.6794102 ],\n",
       "       [ 6.22928681,  9.72328378],\n",
       "       [ 5.66355696,  3.67340317],\n",
       "       [ 5.78414171,  1.89489318],\n",
       "       [ 4.83929426,  7.84534849],\n",
       "       [ 6.37169262,  9.259066  ],\n",
       "       [ 8.12845631,  8.83153769]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy.random as npr\n",
    "X_train = npr.normal(7, 2, (100, 2))\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients are 7 and -2.\n",
    "y_train = 10 + (X_train[:,0] * 7) + (X_train[:,1] * -2) + npr.normal(0, 1, (100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4)** Now test whether you `fit` method generates the correct coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.65758858, -1.25124414])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = MyLinearModel()\n",
    "my_model.fit(X_train, y_train)\n",
    "my_model.coefficients # I'd better get back 7 and -2!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(5)** Now let's make the statisticians proud, and in addition to storing the coefficients, let's store the standard errors for our estimated coefficients as another attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyLinearModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize!\n",
    "        self.coefficients = None\n",
    "        self.standard_errors = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Fit the model using regressors X and outcomes y\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "        X_transposed = np.transpose(X)\n",
    "        self.coefficients = np.linalg.inv(X_transposed @ X) @ (X_transposed @ y)\n",
    "        \n",
    "        # Get standard errors\n",
    "        errors = (y - X @ self.coefficients)\n",
    "        errors_squared = (errors @ np.transpose(errors))\n",
    "        std_estimate = errors_squared / X.shape[0]\n",
    "        errors_times_coefficients_sq_inv = std_estimate * np.linalg.inv(X_transposed @ X)\n",
    "        self.standard_errors = np.sqrt(np.diagonal(errors_times_coefficients_sq_inv))\n",
    "                                 \n",
    "    def predict(self, X_test):\n",
    "        # Make predictions on X_test\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5051385 , 0.04921437, 0.05013119])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = MyLinearModel()\n",
    "my_model.fit(X_train, y_train)\n",
    "my_model.standard_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(6)** Now let's also add an R-squarded attribute to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyLinearModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize!\n",
    "        self.coefficients = None\n",
    "        self.standard_errors = None\n",
    "        self.r_squared = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Fit the model using regressors X and outcomes y\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "        X_transposed = np.transpose(X)\n",
    "        self.coefficients = np.linalg.inv(X_transposed @ X) @ (X_transposed @ y)\n",
    "        \n",
    "        # Get Standard errors\n",
    "        errors = (y - X @ self.coefficients)\n",
    "        errors_squared = (errors @ np.transpose(errors))\n",
    "        std_estimate = errors_squared / X.shape[0]\n",
    "        errors_times_coefficients_sq_inv = std_estimate * np.linalg.inv(X_transposed @ X)\n",
    "        self.standard_errors = np.sqrt(np.diagonal(errors_times_coefficients_sq_inv))\n",
    "                            \n",
    "        # Get Rsquared\n",
    "        self.r_squared = 1 - np.sum(errors_squared) / np.sum( (y - np.mean(y))**2 )\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        # Make predictions on X_test\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9954235071596333"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = MyLinearModel()\n",
    "my_model.fit(X_train, y_train)\n",
    "my_model.r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(7)** Now we'll go ahead and cheat a little. Use `statsmodels` to see if your standard errors and r-squared are correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.995</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>1.055e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 05 Nov 2019</td> <th>  Prob (F-statistic):</th> <td>3.43e-114</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:28:51</td>     <th>  Log-Likelihood:    </th> <td> -138.45</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   282.9</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    97</td>      <th>  BIC:               </th> <td>   290.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>   10.3357</td> <td>    0.513</td> <td>   20.152</td> <td> 0.000</td> <td>    9.318</td> <td>   11.354</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>        <td>    6.9337</td> <td>    0.050</td> <td>  138.758</td> <td> 0.000</td> <td>    6.835</td> <td>    7.033</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>        <td>   -1.9599</td> <td>    0.051</td> <td>  -38.505</td> <td> 0.000</td> <td>   -2.061</td> <td>   -1.859</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 1.679</td> <th>  Durbin-Watson:     </th> <td>   2.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.432</td> <th>  Jarque-Bera (JB):  </th> <td>   1.730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.278</td> <th>  Prob(JB):          </th> <td>   0.421</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.673</td> <th>  Cond. No.          </th> <td>    52.9</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.995\n",
       "Model:                            OLS   Adj. R-squared:                  0.995\n",
       "Method:                 Least Squares   F-statistic:                 1.055e+04\n",
       "Date:                Tue, 05 Nov 2019   Prob (F-statistic):          3.43e-114\n",
       "Time:                        13:28:51   Log-Likelihood:                -138.45\n",
       "No. Observations:                 100   AIC:                             282.9\n",
       "Df Residuals:                      97   BIC:                             290.7\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept     10.3357      0.513     20.152      0.000       9.318      11.354\n",
       "x1             6.9337      0.050    138.758      0.000       6.835       7.033\n",
       "x2            -1.9599      0.051    -38.505      0.000      -2.061      -1.859\n",
       "==============================================================================\n",
       "Omnibus:                        1.679   Durbin-Watson:                   2.143\n",
       "Prob(Omnibus):                  0.432   Jarque-Bera (JB):                1.730\n",
       "Skew:                           0.278   Prob(JB):                        0.421\n",
       "Kurtosis:                       2.673   Cond. No.                         52.9\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf \n",
    "df = pd.DataFrame({'y': y_train, 'x1': X_train[:,0], 'x2': X_train[:,1]})\n",
    "df_model = smf.ols('y ~ x1 + x2', df).fit()\n",
    "df_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(8)** Now implement `predict`! Then test it against your original `X` data -- do you get back something very close to your true `y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyLinearModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        # Initialize!\n",
    "        self.coefficients = None\n",
    "        self.standard_errors = None\n",
    "        self.r_squared = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Fit the model using regressors X and outcomes y\n",
    "        X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "        X_transposed = np.transpose(X)\n",
    "        self.coefficients = np.linalg.inv(X_transposed @ X) @ (X_transposed @ y)\n",
    "        \n",
    "        # Get Standard errors\n",
    "        errors = (y - X @ self.coefficients)\n",
    "        errors_squared = (errors @ np.transpose(errors))\n",
    "        errors_times_coefficients_sq_inv = errors_squared * np.linalg.inv(X_transposed @ X)\n",
    "        self.standard_errors = np.sqrt(np.diagonal(errors_times_coefficients_sq_inv))\n",
    "                            \n",
    "        # Get Rsquared\n",
    "        self.r_squared = 1 - np.sum(errors_squared) / np.sum( (y - np.mean(y))**2 )\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        # Return predicted values. \n",
    "        X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)\n",
    "        return X_test @ self.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = MyLinearModel()\n",
    "my_model.fit(X_train, y_train)\n",
    "y_test = my_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.461264909873366e-16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_test - y_train) / np.mean(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(9)** Finally, create the *option* of fitting the model with or without a constant term. As in `scikit-learn`, make this an option you set during initialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyLinearModel:\n",
    "\n",
    "    def __init__(self, add_constant=True):\n",
    "        # Initialize!\n",
    "        self.coefficients = None\n",
    "        self.standard_errors = None\n",
    "        self.r_squared = None\n",
    "        self.add_constant = add_constant\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Fit the model using regressors X and outcomes y\n",
    "        \n",
    "        # Add constants column if desired\n",
    "        if self.add_constant:\n",
    "            X = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "\n",
    "        X_transposed = np.transpose(X)\n",
    "        self.coefficients = np.linalg.inv(X_transposed @ X) @ (X_transposed @ y)\n",
    "        \n",
    "        # Get Standard errors\n",
    "        errors = (y - X @ self.coefficients)\n",
    "        errors_squared = (errors @ np.transpose(errors))\n",
    "        std_estimate = errors_squared / X.shape[0]\n",
    "        errors_times_coefficients_sq_inv = std_estimate * np.linalg.inv(X_transposed @ X)\n",
    "        self.standard_errors = np.sqrt(np.diagonal(errors_times_coefficients_sq_inv))\n",
    "                            \n",
    "        # Get Rsquared\n",
    "        self.r_squared = 1 - np.sum(errors_squared) / np.sum( (y - np.mean(y))**2 )\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        # Return predicted values. \n",
    "        \n",
    "        # Add constants column if desired\n",
    "        if self.add_constant:\n",
    "            X_test = np.concatenate([np.ones((X_test.shape[0], 1)), X_test], axis=1)\n",
    "\n",
    "        return X_test @ self.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.335717  ,  6.9337087 , -1.95990979])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = MyLinearModel(add_constant=True)\n",
    "my_model.fit(X_train, y_train)\n",
    "my_model.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.65758858, -1.25124414])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model = MyLinearModel(add_constant=False)\n",
    "my_model.fit(X_train, y_train)\n",
    "my_model.coefficients"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
